{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87aa93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63430ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file into pandas DataFrame\n",
    "\n",
    "p3d_diag= pd.read_csv(\"C:\\\\PASC_Pilot\\\\DATA\\\\Post3day_diag_phe.csv\")\n",
    "p3d_diag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008f54fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "p3d_diag['PERSON_ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a3498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Filter out those who U09 to create the positive labels\n",
    "pattern = 'SS_807.3'\n",
    "pasc_diag = p3d_diag[p3d_diag['CONDITION_SOURCE_VALUE'].str.contains(pattern, flags=re.IGNORECASE, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b094f124",
   "metadata": {},
   "outputs": [],
   "source": [
    "pasc_diag['PERSON_ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70f91d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pasc_diag = pd.DataFrame({'PERSON_ID': pasc_diag['PERSON_ID'].unique()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6093771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pasc_diag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd2062",
   "metadata": {},
   "outputs": [],
   "source": [
    "pasc_diag.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e901ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pasc_diag['PASC'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc327f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p3d_diag['dummy'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1732944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_counts = p3d_diag.groupby(['PERSON_ID', 'CONDITION_SOURCE_VALUE_DESCRIPTION']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063ddc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk):\n",
    "    # Group by PERSON_ID and CONDITION_SOURCE_VALUE_DESCRIPTION to get the counts\n",
    "    grouped_counts = chunk.groupby(['PERSON_ID', 'CONDITION_SOURCE_VALUE_DESCRIPTION']).size().reset_index(name='counts')\n",
    "    \n",
    "    # Pivot the grouped data\n",
    "    chunk_pivot = grouped_counts.pivot(index='PERSON_ID', \n",
    "                                       columns='CONDITION_SOURCE_VALUE_DESCRIPTION', \n",
    "                                       values='counts').fillna(0)\n",
    "    \n",
    "    return chunk_pivot\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 1000  # You can adjust this based on your system's memory\n",
    "\n",
    "# Create a list to hold the results of each chunk\n",
    "results = []\n",
    "\n",
    "# Split the data into chunks and process each chunk\n",
    "for start in range(0, len(p3d_diag), chunk_size):\n",
    "    chunk = p3d_diag.iloc[start:start+chunk_size]\n",
    "    results.append(process_chunk(chunk))\n",
    "\n",
    "# Combine the results\n",
    "diag_features = pd.concat(results).groupby(level=0).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b457e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b89e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the new demographic file into pandas DataFrame\n",
    "dem_df= pd.read_csv(\"C:\\\\PASC_Pilot\\\\DATA\\\\new_dem.csv\")\n",
    "dem_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d17337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the demographic file by p3d_diag (after 30 days patients)\n",
    "\n",
    "filtered_dem = dem_df[dem_df['PERSON_ID'].isin(p3d_diag['PERSON_ID'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fd4829",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dem['PERSON_ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ebbafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read visit file into pandas DataFrame\n",
    "vs_df = pd.read_csv(\"C:\\PASC_Pilot\\DATA\\Visit_count.csv\")\n",
    "vs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c39d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b90dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the visit file by p3_diag\n",
    "\n",
    "filtered_vs = vs_df[vs_df['PERSON_ID'].isin(p3d_diag['PERSON_ID'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d06d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_vs['PERSON_ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c8df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df7d51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the dataframes (demographic and diagnoses) \n",
    "mgd_df = pd.merge(filtered_dem, diag_features,  on='PERSON_ID', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf663dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the dataframe with visit count\n",
    "mgd_df2 = pd.merge(mgd_df, filtered_vs,  on='PERSON_ID', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2740c215",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgd_df2['VISIT_START_DATE'] =  mgd_df2['VISIT_START_DATE'].fillna(0)\n",
    "mgd_df2 = mgd_df2.rename(columns={'VISIT_START_DATE': 'visit_count'})\n",
    "mgd_df2 = mgd_df2.rename(columns={'GENDER_CONCEPT_VALUE': 'Gender'})\n",
    "mgd_df2 = mgd_df2.rename(columns={'RACE_SOURCE_VALUE': 'Race'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfed6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode all variables\n",
    "mgd_df2 = pd.get_dummies(mgd_df2, columns=['Gender'])\n",
    "mgd_df2 = pd.get_dummies(mgd_df2, columns=['Race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf577fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgd_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40540002",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgd_df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484a49f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = mgd_df2.merge(pasc_diag, on=\"PERSON_ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69710948",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebff51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"PASC\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c18dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c847550",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop('PERSON_ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0384d075",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"PASC\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7813e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4775d643",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(['Sars-CoV-2*','Post COVID-19 condition*','age_group',\n",
    "                           'Personal history of COVID-19',\n",
    "                           'Contact with and (suspected) exposure to covid-19'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100e522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all columns are integers\n",
    "merged_df = merged_df.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140e0753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_column_name(column_name):\n",
    "    # Replace unsupported characters with an underscore\n",
    "    for ch in [':', ',', '{', '}', '\\n', '[', ']', '\"', '\\\\']:\n",
    "        column_name = column_name.replace(ch, '_')\n",
    "\n",
    "    # Remove leading and trailing underscores\n",
    "    column_name = column_name.strip('_')\n",
    "\n",
    "    return column_name\n",
    "\n",
    "def make_unique(column_names):\n",
    "    seen = set()\n",
    "    unique_column_names = []\n",
    "    for col in column_names:\n",
    "        col_cleaned = clean_column_name(col)\n",
    "        while col_cleaned in seen:\n",
    "            col_cleaned += \"_1\"\n",
    "        unique_column_names.append(col_cleaned)\n",
    "        seen.add(col_cleaned)\n",
    "    return unique_column_names\n",
    "\n",
    "\n",
    "merged_df.columns = make_unique(merged_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86037978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-select some features\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare data\n",
    "X = merged_df.drop('PASC', axis=1)\n",
    "y = merged_df['PASC']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create a LightGBM classifier and fit it to the training data\n",
    "lgb_classifier = lgb.LGBMClassifier(objective='binary', boosting_type='gbdt', n_jobs=-1, metric='binary_logloss')\n",
    "lgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Get the feature importances\n",
    "feature_importances = lgb_classifier.feature_importances_\n",
    "\n",
    "# Create DataFrame with the feature importances\n",
    "feature_importance_df = pd.DataFrame({'feature': X.columns, 'importance': feature_importances})\n",
    "feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "# Select a certain number of top features based on their importance\n",
    "top_n = 500\n",
    "top_features = feature_importance_df['feature'].head(top_n).tolist()\n",
    "\n",
    "# Create DataFrame with the top features and the target variable\n",
    "filtered_df = merged_df[top_features + ['PASC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116db569",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d7b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your data\n",
    "X = filtered_df.drop('PASC', axis=1)\n",
    "y = filtered_df['PASC']\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 1000\n",
    "\n",
    "# Calculate the correlation between each feature and the target variable in batches\n",
    "correlations = pd.Series(dtype=float)\n",
    "for i in range(0, len(X), batch_size):\n",
    "    data_batch = pd.concat([X.iloc[i:i + batch_size], y.iloc[i:i + batch_size]], axis=1)\n",
    "    data_batch.columns = list(X.columns) + ['PASC']\n",
    "    batch_correlations = data_batch.corrwith(data_batch['PASC']).abs()\n",
    "    correlations = correlations.add(batch_correlations, fill_value=0)\n",
    "\n",
    "# Divide the correlations by the number of batches to get the average\n",
    "correlations /= (len(X) // batch_size)\n",
    "\n",
    "# Sort the correlations\n",
    "correlations = correlations.sort_values(ascending=False)\n",
    "\n",
    "# Set a correlation threshold to filter out highly correlated features\n",
    "correlation_threshold = 0.7\n",
    "\n",
    "# Get the names of the features that have a correlation above the threshold\n",
    "highly_correlated_features = correlations[correlations > correlation_threshold].index.tolist()\n",
    "\n",
    "# Remove the target variable from the list\n",
    "highly_correlated_features.remove('PASC')\n",
    "\n",
    "# Print the highly correlated features\n",
    "print(f\"Highly correlated features (correlation > {correlation_threshold}):\")\n",
    "for feature in highly_correlated_features:\n",
    "    print(f\"{feature}: {correlations[feature]}\")\n",
    "\n",
    "# Remove the highly correlated features from your dataset\n",
    "X_filtered = X.drop(highly_correlated_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6df5f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = pd.concat([X_filtered, y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e118086",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d69cbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed03edbe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, confusion_matrix\n",
    "from sklearn.ensemble import StackingClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import average_precision_score, plot_precision_recall_curve\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "X = filtered_df.drop('PASC', axis=1)\n",
    "y = filtered_df['PASC']\n",
    "\n",
    "# Store feature names before scaling\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Resampling only on training data using ADASYN sampling strategy\n",
    "adasyn = ADASYN(random_state=42, sampling_strategy=0.75)\n",
    "X_train, y_train = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create a LightGBM classifier with adjusted hyperparameters\n",
    "lgb_classifier = lgb.LGBMClassifier(\n",
    "    objective='binary', \n",
    "    boosting_type='gbdt', \n",
    "    n_jobs=-1, \n",
    "    metric='binary_logloss', \n",
    "    min_data_in_leaf=50,  \n",
    "    class_weight='balanced', \n",
    "    reg_lambda=2.0,\n",
    "    reg_alpha=0.5\n",
    ")\n",
    "\n",
    "# Hyperparameter search space\n",
    "param_dist = {\n",
    "    'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'min_child_samples': [5, 10, 20, 50, 100],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1, 1.5],\n",
    "    'reg_lambda': [0, 0.1, 0.5, 1, 2, 3],\n",
    "    'max_depth': [-1, 5, 10, 20, 30, 40],\n",
    "    'num_leaves': [7, 15, 31, 63, 127],\n",
    "    'learning_rate': [0.001, 0.01, 0.05, 0.1],\n",
    "    'n_estimators': [50, 100, 200, 300]\n",
    "}\n",
    "# Initialize RandomizedSearchCV\n",
    "stratified_shuffle_split = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "random_search = RandomizedSearchCV(lgb_classifier, param_distributions=param_dist, n_iter=100, cv=stratified_shuffle_split, n_jobs=-1)\n",
    "\n",
    "## Timing the training for best LightGBM model\n",
    "start_time_lgb = time.time()\n",
    "\n",
    "# Fit the model with early stopping\n",
    "random_search.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=100)\n",
    "\n",
    "end_time_lgb = time.time()\n",
    "elapsed_time_lgb = end_time_lgb - start_time_lgb\n",
    "print(f\"Time taken to train the best LightGBM model: {elapsed_time_lgb:.2f} seconds\")\n",
    "\n",
    "# Get the best estimator for LightGBM\n",
    "best_lgb_model = random_search.best_estimator_\n",
    "\n",
    "# Additional models\n",
    "gradient_boosting_model = GradientBoostingClassifier(n_estimators=100)\n",
    "bagging_lgb = BaggingClassifier(base_estimator=lgb_classifier, n_estimators=10, random_state=42)\n",
    "\n",
    "# Base models for stacking\n",
    "base_models = [\n",
    "    ('lgbm', random_search.best_estimator_),\n",
    "    ('svm', SVC(probability=True)),\n",
    "    ('gradient_boosting', gradient_boosting_model),\n",
    "    ('bagging_lgb', bagging_lgb)\n",
    "]\n",
    "\n",
    "# Meta-model for stacking\n",
    "meta_model = BaggingClassifier(base_estimator=lgb_classifier, n_estimators=10, random_state=42)\n",
    "\n",
    "# Create the stacking model\n",
    "stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model)\n",
    "\n",
    "# Timing the training for the stacking model\n",
    "start_time_stacking = time.time()\n",
    "\n",
    "# Fit the stacking model\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "end_time_stacking = time.time()\n",
    "elapsed_time_stacking = end_time_stacking - start_time_stacking\n",
    "print(f\"Time taken to train the Stacking model: {elapsed_time_stacking:.2f} seconds\")\n",
    "\n",
    "\n",
    "# Display feature importances\n",
    "importances = random_search.best_estimator_.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "# Create a dictionary mapping feature names to their importances\n",
    "feature_importance_dict = dict(zip(feature_names, importances))\n",
    "\n",
    "# Sort the dictionary by importance values\n",
    "sorted_features = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Feature Ranking:\")\n",
    "for rank, (feature, importance) in enumerate(sorted_features, 1):\n",
    "    print(f\"{rank}. {feature}: {importance}\")\n",
    "\n",
    "\n",
    "# Adjust threshold for classification\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "thresholds = [i/100 for i in range(20, 81)]\n",
    "\n",
    "y_pred_proba = random_search.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "y_pred = (y_pred_proba >= best_threshold).astype(int)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"LBG Model Performance with Adjusted Threshold:\")\n",
    "print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, ROC AUC: {roc_auc}\\n\")\n",
    "print(f\"F1-score: {f1}\")\n",
    "\n",
    "# Make predictions with the stacking model\n",
    "y_pred = stacking_model.predict(X_test)\n",
    "y_pred_proba = stacking_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Performance metrics for the stacking model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Stacking Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, ROC AUC: {roc_auc}\\n\")\n",
    "print(f\"F1-score: {f1}\")\n",
    "\n",
    "print(f\"Best Threshold: {best_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28204d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd97bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the numpy arrays back to DataFrames to preserve feature names\n",
    "X_test = pd.DataFrame(X_test, columns=feature_names)\n",
    "\n",
    "# Import SHAP\n",
    "import shap\n",
    "\n",
    "# Create a Tree explainer for the best LightGBM model\n",
    "explainer = shap.TreeExplainer(best_lgb_model)\n",
    "\n",
    "# Calculate SHAP values for the test data\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Plot summary_plot for the positive class (usually class index 1)\n",
    "shap.summary_plot(shap_values[1], X_test, max_display=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d8e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Create a SHAP Tree explainer for the best LightGBM model\n",
    "explainer = shap.TreeExplainer(best_lgb_model)\n",
    "\n",
    "# Calculate SHAP values for the test data\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Create a SHAP Explanation object for the first instance in the test set\n",
    "explanation = shap.Explanation(values=shap_values[1][0], \n",
    "                               base_values=explainer.expected_value[1], \n",
    "                               data=X_test.iloc[0], \n",
    "                               feature_names=X_test.columns.tolist())\n",
    "\n",
    "# Visualize the first prediction's explanation using a waterfall plot\n",
    "shap.plots.waterfall(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d70ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_ids_to_select = [1991813, 3035829, 1990878, 1814700, 2270188, 2143561, 1586566, 2054655, 1305342, 1251904]\n",
    "# Filter the DataFrame\n",
    "data_selected = mgd_df2[mgd_df2['PERSON_ID'].isin(person_ids_to_select)]\n",
    "merged_df2 = data_selected.drop('PERSON_ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a72f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df2 = merged_df2.drop(['Sars-CoV-2*','Post COVID-19 condition*'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586b63f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_column_name(column_name):\n",
    "    # Replace unsupported characters with an underscore\n",
    "    for ch in [':', ',', '{', '}', '\\n', '[', ']', '\"', '\\\\']:\n",
    "        column_name = column_name.replace(ch, '_')\n",
    "\n",
    "    # Remove leading and trailing underscores\n",
    "    column_name = column_name.strip('_')\n",
    "\n",
    "    return column_name\n",
    "\n",
    "def make_unique(column_names):\n",
    "    seen = set()\n",
    "    unique_column_names = []\n",
    "    for col in column_names:\n",
    "        col_cleaned = clean_column_name(col)\n",
    "        while col_cleaned in seen:\n",
    "            col_cleaned += \"_1\"\n",
    "        unique_column_names.append(col_cleaned)\n",
    "        seen.add(col_cleaned)\n",
    "    return unique_column_names\n",
    "\n",
    "# Replace 'your_dataframe' with the actual variable name of your dataframe\n",
    "merged_df2.columns = make_unique(merged_df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6d962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature importance (select the same top features used during training)\n",
    "filtered_unlabeled_data = merged_df2[top_features]\n",
    "\n",
    "#filtered_unlabeled_data = filtered_df.drop('Post covid-19 condition_ unspecified',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1f2e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model to the unseen data\n",
    "unseen_data_predictions1 = best_lgb_model.predict(filtered_unlabeled_data)\n",
    "\n",
    "# If you want predicted probabilities instead of class labels, use the following:\n",
    "unseen_data_probabilities1 = best_lgb_model.predict_proba(filtered_unlabeled_data)[:, 1]\n",
    "\n",
    "print(unseen_data_predictions1)\n",
    "print(unseen_data_probabilities1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4d58ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model to the unseen data\n",
    "unseen_data_predictions = stacking_model.predict(filtered_unlabeled_data)\n",
    "\n",
    "\n",
    "unseen_data_probabilities = stacking_model.predict_proba(filtered_unlabeled_data)[:, 1]\n",
    "\n",
    "print(unseen_data_predictions)\n",
    "print(unseen_data_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e696d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the original data and the predicted labels\n",
    "unlabeled_data_with_predictions = pd.DataFrame(data_selected['PERSON_ID'])\n",
    "unlabeled_data_with_predictions['predicted_label'] = unseen_data_predictions\n",
    "\n",
    "# For prediction probabilities\n",
    "unlabeled_data_with_predictions['predicted_probability'] = unseen_data_probabilities\n",
    "\n",
    "# Now you can view the results by printing the DataFrame\n",
    "unlabeled_data_with_predictions.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b60b503",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.force(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0, :], matplotlib = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87348967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
